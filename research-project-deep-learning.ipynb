{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1136791,"sourceType":"datasetVersion","datasetId":526736}],"dockerImageVersionId":30055,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-11T02:44:01.653893Z","iopub.execute_input":"2024-07-11T02:44:01.654485Z","iopub.status.idle":"2024-07-11T02:44:01.668286Z","shell.execute_reply.started":"2024-07-11T02:44:01.654378Z","shell.execute_reply":"2024-07-11T02:44:01.667173Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-translated-urdu-reviews/imdb_urdu_reviews_test.csv\n/kaggle/input/imdb-dataset-of-50k-movie-translated-urdu-reviews/imdb_urdu_reviews_train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Install  Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install urduhack","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:46:00.306077Z","iopub.execute_input":"2024-07-11T02:46:00.306566Z","iopub.status.idle":"2024-07-11T02:46:14.646731Z","shell.execute_reply.started":"2024-07-11T02:46:00.306511Z","shell.execute_reply":"2024-07-11T02:46:14.645257Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting urduhack\n  Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n\u001b[K     |████████████████████████████████| 105 kB 5.8 MB/s eta 0:00:01\n\u001b[?25hCollecting tf2crf\n  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\nRequirement already satisfied: Click~=7.1 in /opt/conda/lib/python3.7/site-packages (from urduhack) (7.1.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from urduhack) (2020.11.13)\nCollecting tensorflow-datasets~=3.1\n  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n\u001b[K     |████████████████████████████████| 3.4 MB 15.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (0.18.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.1.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (0.10.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.19.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (2.25.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.15.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (4.55.1)\nRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (20.3.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (0.3.3)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (3.14.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (1.12.1)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (0.27.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.0.4)\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack) (1.52.0)\nRequirement already satisfied: tensorflow-addons>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from tf2crf->urduhack) (0.12.0)\nRequirement already satisfied: tensorflow>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tf2crf->urduhack) (2.4.0)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.10.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.36.2)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.12)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.7.4.3)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.4.0)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.3.3)\nRequirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.32.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.1.2)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.4.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.8.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.0.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (49.6.0.post20201009)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.3.3)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.24.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (4.6)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.0.1)\nRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack) (2.10.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.4.0)\nInstalling collected packages: tf2crf, tensorflow-datasets, urduhack\n  Attempting uninstall: tensorflow-datasets\n    Found existing installation: tensorflow-datasets 3.0.0\n    Uninstalling tensorflow-datasets-3.0.0:\n      Successfully uninstalled tensorflow-datasets-3.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-cloud 0.1.11 requires tensorflow-datasets<3.1.0, but you have tensorflow-datasets 3.2.1 which is incompatible.\u001b[0m\nSuccessfully installed tensorflow-datasets-3.2.1 tf2crf-0.1.33 urduhack-1.1.1\n\u001b[33mWARNING: You are using pip version 21.0; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Import Plotting Libararies\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Import Data Preprocessing Libraries \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Machine Learning Models\nfrom sklearn import svm  \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\n\n# Model Evaluation Libraries\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:51:16.939432Z","iopub.execute_input":"2024-07-11T02:51:16.939954Z","iopub.status.idle":"2024-07-11T02:51:16.958217Z","shell.execute_reply.started":"2024-07-11T02:51:16.939909Z","shell.execute_reply":"2024-07-11T02:51:16.956642Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Urduhack\n\nUrduhack is a NLP library for urdu language. It comes with a lot of battery included features to help you process Urdu data in the easiest way possible.\n \nhttps://docs.urduhack.com/en/stable/\n\n","metadata":{}},{"cell_type":"code","source":"import urduhack\nurduhack.download()\nfrom urduhack.normalization import normalize\nfrom urduhack.preprocessing import normalize_whitespace, remove_punctuation, remove_accents, replace_urls, replace_emails, replace_numbers, replace_currency_symbols, remove_english_alphabets","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:51:19.957554Z","iopub.execute_input":"2024-07-11T02:51:19.958014Z","iopub.status.idle":"2024-07-11T02:51:31.025315Z","shell.execute_reply.started":"2024-07-11T02:51:19.957974Z","shell.execute_reply":"2024-07-11T02:51:31.023754Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/urduhack/resources/releases/download/word_tokenizer/word_tokenizer.zip\n36790272/36788015 [==============================] - 0s 0us/step\nDownloading data from https://github.com/urduhack/resources/releases/download/pos_tagger/pos_tagger.zip\n2768896/2761433 [==============================] - 0s 0us/step\nDownloading data from https://github.com/urduhack/resources/releases/download/ner/ner.zip\n11730944/11723346 [==============================] - 0s 0us/step\nDownloading data from https://github.com/urduhack/resources/releases/download/lemmatizer/ur_lemma_lookup.zip\n90112/89078 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Load Dataset\nDataset is available in 2 sets:\n* Training\n* Testing","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-translated-urdu-reviews/imdb_urdu_reviews_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-translated-urdu-reviews/imdb_urdu_reviews_test.csv\")\n\ntrain_data.head(), test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:52:46.948763Z","iopub.execute_input":"2024-07-11T02:52:46.949327Z","iopub.status.idle":"2024-07-11T02:52:50.542923Z","shell.execute_reply.started":"2024-07-11T02:52:46.949282Z","shell.execute_reply":"2024-07-11T02:52:50.541552Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(                                              review sentiment\n 0  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...  positive\n 1  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...  negative\n 2  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...  positive\n 3  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...  negative\n 4  یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...  positive,\n                                               review sentiment\n 0  یہ بے گھر خواتین کے بارے میں ایک دستاویزی فلم ...  negative\n 1  بالکل بھی اچھ ،ی کام نہیں کیا گیا ، پوری فلم ص...  negative\n 2  یہ عجیب بات ہے کہ کچھ لوگوں کا کیا حشر ہوتا ہے...  negative\n 3  اور یہ خاص طور پر وکیلوں اور پولیس اہلکاروں کے...  positive\n 4  پہلے ، ایک وضاحت: میری سرخی کے باوجود ، میں اس...  positive)"},"metadata":{}}]},{"cell_type":"code","source":"# Combine Both Files to Preprocess \ndata =  pd.concat([train_data, test_data]).reset_index(drop=True)\nprint(data.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:55:59.116972Z","iopub.execute_input":"2024-07-11T02:55:59.117503Z","iopub.status.idle":"2024-07-11T02:55:59.143457Z","shell.execute_reply.started":"2024-07-11T02:55:59.117461Z","shell.execute_reply":"2024-07-11T02:55:59.141977Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(50000, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Make Copy of dataset so we dont have to load again and again","metadata":{}},{"cell_type":"code","source":"# Make copy of a dataset\ndf =  data.copy()\ndf","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:56:08.683027Z","iopub.execute_input":"2024-07-11T02:56:08.683436Z","iopub.status.idle":"2024-07-11T02:56:08.707786Z","shell.execute_reply.started":"2024-07-11T02:56:08.683399Z","shell.execute_reply":"2024-07-11T02:56:08.705875Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                  review sentiment\n0      میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...  positive\n1      چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...  negative\n2      ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...  positive\n3      مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...  negative\n4      یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...  positive\n...                                                  ...       ...\n49995  اگر آپ چیخنا چاہتے ہو یا بڑے اسٹوڈیو ہارر پراڈ...  positive\n49996  براہ راست یہ ایک محض ایک چھوٹی سی چھوٹی چھوٹی ...  positive\n49997  میں نے اس فلم کو کل رات آدھی رات کو چپکے سے پہ...  negative\n49998  دیکھنا کوئی آسان فلم نہیں ہے - یہ ساڑھے تین گھ...  positive\n49999  ناگرا قدامت پسند ہندوستانی خاندان سے تعلق رکھت...  positive\n\n[50000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>اگر آپ چیخنا چاہتے ہو یا بڑے اسٹوڈیو ہارر پراڈ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>براہ راست یہ ایک محض ایک چھوٹی سی چھوٹی چھوٹی ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>میں نے اس فلم کو کل رات آدھی رات کو چپکے سے پہ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>دیکھنا کوئی آسان فلم نہیں ہے - یہ ساڑھے تین گھ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>ناگرا قدامت پسند ہندوستانی خاندان سے تعلق رکھت...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We have now 50,000 records available in our dataset. The size of dataset is good and we can build very good predictive model using this data.","metadata":{}},{"cell_type":"markdown","source":"Lets see the distribution of label column which is sentiment.","metadata":{}},{"cell_type":"code","source":"sns.countplot( x = 'sentiment', data = df );","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:56:15.597419Z","iopub.execute_input":"2024-07-11T02:56:15.597888Z","iopub.status.idle":"2024-07-11T02:56:15.838761Z","shell.execute_reply.started":"2024-07-11T02:56:15.597850Z","shell.execute_reply":"2024-07-11T02:56:15.837534Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"We can see that there are only two classes in our dataset:\n\n* Positive means the review holds positive sentiment.\n* Negative means the review holds negative sentiment.\n\nAlso the class is very balanced. So, it will be easy for us to build any model.","metadata":{}},{"cell_type":"markdown","source":"## 4. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Label Encoding of Target Variable\n\nEncode the target label sentiment.","metadata":{}},{"cell_type":"code","source":"# Encode the labels\nle = LabelEncoder()\nle.fit(df['sentiment'])\ndf['encoded_sentiments'] = le.transform(df['sentiment'])\ndf['encoded_sentiments'].head()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:59:23.524310Z","iopub.execute_input":"2024-07-11T02:59:23.524787Z","iopub.status.idle":"2024-07-11T02:59:23.556565Z","shell.execute_reply.started":"2024-07-11T02:59:23.524746Z","shell.execute_reply":"2024-07-11T02:59:23.554904Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0    1\n1    0\n2    1\n3    0\n4    1\nName: encoded_sentiments, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### 4.2 Apply urduhack preprocessing\nNow we will apply text cleaning modules from Urdu Hack Library","metadata":{}},{"cell_type":"code","source":"df['review'] = df['review'].apply(normalize) # To normalize some text, all you need to do pass unicode text. It will return a str with normalized characters both single and combined, proper spaces after digits and punctuations and diacritics(Zabar - Paish) removed.\n# df['review'] = df['review'].apply(remove_punctuation) # Remove punctuation from text by removing all instances of marks. marks=',;:'\ndf['review'] = df['review'].apply(remove_accents) # Remove accents from any accented unicode characters in text str, either by transforming them into ascii equivalents or removing them entirely.\ndf['review'] = df['review'].apply(replace_urls) # Replace all URLs in text str with replace_with str.\ndf['review'] = df['review'].apply(replace_emails) # Replace all emails in text str with replace_with str.\n# df['review'] = df['review'].apply(replace_numbers) # Replace all numbers in text str with replace_with str.\ndf['review'] = df['review'].apply(replace_currency_symbols) # Replace all currency symbols in text str with string specified by replace_with str.\n# df['review'] = df['review'].apply(remove_english_alphabets) # Removes English words and digits from a text\ndf['review'] = df['review'].apply(normalize_whitespace) ## Given text str, replace one or more spacings with a single space, and one or more linebreaks with a single newline. Also strip leading/trailing whitespace.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using publically available set of Urdu Text Stopwords we will remove stop words from our text.","metadata":{}},{"cell_type":"code","source":"# Remove stop words from text\nfrom typing import FrozenSet\n\n# Urdu Language Stop words list\nSTOP_WORDS: FrozenSet[str] = frozenset(\"\"\"\n آ آئی آئیں آئے آتا آتی آتے آس آمدید آنا آنسہ آنی آنے آپ آگے آہ آہا آیا اب ابھی ابے\n ارے اس اسکا اسکی اسکے اسی اسے اف افوہ البتہ الف ان اندر انکا انکی انکے انہوں انہی انہیں اوئے اور اوپر\n اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکثر اگر اگرچہ اہاہا ایسا ایسی ایسے ایک بائیں بار بارے بالکل باوجود باہر\n بج بجے بخیر بشرطیکہ بعد بعض بغیر بلکہ بن بنا بناؤ بند بڑی بھر بھریں بھی بہت بہتر تاکہ تاہم تب تجھ\n تجھی تجھے ترا تری تلک تم تمام تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے تیرا تیری تیرے\n جا جاؤ جائیں جائے جاتا جاتی جاتے جانی جانے جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی جیسا\n جیسوں جیسی جیسے حالانکہ حالاں حصہ حضرت خاطر خالی خواہ خوب خود دائیں درمیان دریں دو دوران دوسرا دوسروں دوسری دوں\n دکھائیں دی دیئے دیا دیتا دیتی دیتے دیر دینا دینی دینے دیکھو دیں دیے دے ذریعے رکھا رکھتا رکھتی رکھتے رکھنا رکھنی\n رکھنے رکھو رکھی رکھے رہ رہا رہتا رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی\n سراسر سمیت سوا سوائے سکا سکتا سکتے سہ سہی سی سے شاید شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور علاوہ عین\n فقط فلاں فی قبل قطا لئے لائی لائے لاتا لاتی لاتے لانا لانی لانے لایا لو لوجی لوگوں لگ لگا لگتا\n لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن لیں لیے لے ماسوا مت مجھ مجھی مجھے محترم محترمہ محض\n مرا مرحبا مری مرے مزید مس مسز مسٹر مطابق مل مکرمی مگر مگھر مہربانی میرا میروں میری میرے میں نا نزدیک\n نما نہ نہیں نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ وغیرہ ولے وگرنہ وہ وہاں\n وہی وہیں ویسا ویسے ویں پاس پایا پر پس پلیز پون پونی پونے پھر پہ پہلا پہلی پہلے پیر پیچھے چاہئے\n چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ چکی چکیں چکے ڈالنا ڈالنی ڈالنے ڈالے کئے کا کاش کب کبھی\n کدھر کر کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس کسی کسے کم کن کنہیں کو کوئی کون کونسا\n کونسے کچھ کہ کہا کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے کے گئی گئے گا گنا\n گو گویا گی گیا ہائیں ہائے ہاں ہر ہرچند ہرگز ہم ہمارا ہماری ہمارے ہمی ہمیں ہو ہوئی ہوئیں ہوئے ہوا\n ہوبہو ہوتا ہوتی ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\n\"\"\".split())\n\n\ndef remove_stopwords(text: str):\n    return \" \".join(word for word in text.split() if word not in STOP_WORDS)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:00:00.428264Z","iopub.execute_input":"2024-07-11T03:00:00.428764Z","iopub.status.idle":"2024-07-11T03:00:00.440270Z","shell.execute_reply.started":"2024-07-11T03:00:00.428717Z","shell.execute_reply":"2024-07-11T03:00:00.438399Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"len(STOP_WORDS)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:00:03.422219Z","iopub.execute_input":"2024-07-11T03:00:03.422768Z","iopub.status.idle":"2024-07-11T03:00:03.431114Z","shell.execute_reply.started":"2024-07-11T03:00:03.422714Z","shell.execute_reply":"2024-07-11T03:00:03.429520Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"430"},"metadata":{}}]},{"cell_type":"code","source":"df[['review']].head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T02:59:56.384426Z","iopub.execute_input":"2024-07-11T02:59:56.384957Z","iopub.status.idle":"2024-07-11T02:59:56.413102Z","shell.execute_reply.started":"2024-07-11T02:59:56.384913Z","shell.execute_reply":"2024-07-11T02:59:56.411448Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                              review\n0  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...\n1  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...\n2  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...\n3  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...\n4  یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...\n5  مجھے اس فلم کا بیشتر حصہ پسند آیا۔ جیسا کہ دوس...\n6  ٹھیک ہے ، شاید یہ آسکر کا مستحق نہیں ہے۔ یا گو...\n7  میں نے اسے سائنس فائی چینل پر دیکھا۔ یہ پہلے و...\n8  یہ فلم ایک ناقص مووی تھی۔ پلاٹ خراب تھا اور کا...\n9  یہ ایم جی ایم اور فرینک سناتراس بدترین فلموں م...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے اب...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>مجھے اس فلم کا بیشتر حصہ پسند آیا۔ جیسا کہ دوس...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ٹھیک ہے ، شاید یہ آسکر کا مستحق نہیں ہے۔ یا گو...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>میں نے اسے سائنس فائی چینل پر دیکھا۔ یہ پہلے و...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>یہ فلم ایک ناقص مووی تھی۔ پلاٹ خراب تھا اور کا...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>یہ ایم جی ایم اور فرینک سناتراس بدترین فلموں م...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from urduhack.models.lemmatizer import lemmatizer\ndef lemitizeStr(str):\n    lemme_str = \"\"\n    temp = lemmatizer.lemma_lookup(str)\n    for t in temp:\n        lemme_str += t[0] + \" \"\n    \n    return lemme_str","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:01:31.934602Z","iopub.execute_input":"2024-07-11T03:01:31.935120Z","iopub.status.idle":"2024-07-11T03:01:31.941859Z","shell.execute_reply.started":"2024-07-11T03:01:31.935075Z","shell.execute_reply":"2024-07-11T03:01:31.940363Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'] =  df['review'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:01:36.184382Z","iopub.execute_input":"2024-07-11T03:01:36.184806Z","iopub.status.idle":"2024-07-11T03:01:40.412310Z","shell.execute_reply.started":"2024-07-11T03:01:36.184768Z","shell.execute_reply":"2024-07-11T03:01:40.410739Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df['lemmatized_text'] = df['review'].apply(lemitizeStr)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:01:40.415622Z","iopub.execute_input":"2024-07-11T03:01:40.416192Z","iopub.status.idle":"2024-07-11T03:01:46.176168Z","shell.execute_reply.started":"2024-07-11T03:01:40.416137Z","shell.execute_reply":"2024-07-11T03:01:46.174948Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df['review'][2], df['lemmatized_text'][2]","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:01:59.938287Z","iopub.execute_input":"2024-07-11T03:01:59.938726Z","iopub.status.idle":"2024-07-11T03:01:59.946228Z","shell.execute_reply.started":"2024-07-11T03:01:59.938684Z","shell.execute_reply":"2024-07-11T03:01:59.945121Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ، طیارے ملتے ، جگہ جارہا عرصہ ترک کردیا ، بیوی بچ leftہ چھوڑا تھا۔ ذہنی معذور لڑکے قتل الزام ہے۔ دیکھتے ، اخبار عنوان پکڑا عورت پڑھ ہے۔ بے رحمی ، پوچھتی اخبار رکھ ، خاتون دوسرے حصے پیش ہے۔ ٹھیک ، پوچھا ، عورت چاہتی ، حص giveہ پڑھ ہے۔ میتھیو ریان ہوج پریشان فلم لکھا ہدایت معاشرے کئی عکاسی ہے۔ در حقیقت ، ہوج اشارہ غلط ہے۔ فلم لیلینڈ نوعمر عمر پیش ، خیالی حقیقت فرق کرسکتا ہے۔ بات عیاں انسان قتل ، پیارے معصوم لڑکے مستحق ، ، خاندان مقتول لڑکے کنبہ مہلک نتائج برآمد گے۔ حقیقت ، لیلینڈ پتہ جرم ارتکاب چیز ترغیب توبہ ہے۔ ظاہر والدین طلاق لیلینڈ صدمہ پہنچا ہے۔ باپ متلو .ن آدمی پرواہ تھا۔ پرل ، نوعمر حراستی مرکز استاد نوجوان ہنگامہ دیکھتا مدد چاہتا ، بدقسمتی ، موقع ہے۔ فلم اچھی بات ڈان چیڈل ، عمدہ اداکار ہمیشہ بچاتا ہے۔ . ہوج ہدایت کاری تحت جوڑا کاسٹ اچھ workا کام ہے۔ کیون اسپیس ملزم قاتل مغرور والد کردار ادا اچھا موقع تکبر بیوقوف نفرت ہیں۔ تھوڑی ، فلم جوابات زیادہ سوالات ہے۔',\n 'معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ، طیارے ملتے ، جگہ جارہا عرصہ ترک کردیا ، بیوی بچ leftہ چھوڑا تھا۔ ذہنی معذور لڑکے قتل الزام ہے۔ دیکھتے ، اخبار عنوان پکڑا عورت پڑھ ہے۔ بے رحمی ، پوچھتی اخبار رکھ ، خاتون دوسرے حصے پیش ہے۔ ٹھیک ، پوچھا ، عورت چاہتی ، حص giveہ پڑھ ہے۔ میتھیو ریان ہوج پریشان فلم لکھا ہدایت معاشرے کئی عکاسی ہے۔ در حقیقت ، ہوج اشارہ غلط ہے۔ فلم لیلینڈ نوعمر عمر پیش ، خیالی حقیقت فرق کرسکتا ہے۔ بات عیاں انسان قتل ، پیارے معصوم لڑکے مستحق ، ، خاندان مقتول لڑکے کنبہ مہلک نتائج برآمد گے۔ حقیقت ، لیلینڈ پتہ جرم ارتکاب چیز ترغیب توبہ ہے۔ ظاہر والدین طلاق لیلینڈ صدمہ پہنچا ہے۔ باپ متلو .ن آدمی پرواہ تھا۔ پرل ، نوعمر حراستی مرکز استاد نوجوان ہنگامہ دیکھتا مدد چاہتا ، بدقسمتی ، موقع ہے۔ فلم اچھی بات ڈان چیڈل ، عمدہ اداکار ہمیشہ بچاتا ہے۔ . ہوج ہدایت کاری تحت جوڑا کاسٹ اچھ workا کام ہے۔ کیون اسپیس ملزم قاتل مغرور والد کردار ادا اچھا موقع تکبر بیوقوف نفرت ہیں۔ تھوڑی ، فلم جوابات زیادہ سوالات ہے۔ ')"},"metadata":{}}]},{"cell_type":"code","source":"df[['review', 'lemmatized_text']].head(10)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:02:04.975937Z","iopub.execute_input":"2024-07-11T03:02:04.976441Z","iopub.status.idle":"2024-07-11T03:02:05.007662Z","shell.execute_reply.started":"2024-07-11T03:02:04.976393Z","shell.execute_reply":"2024-07-11T03:02:05.005963Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                              review  \\\n0  80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...   \n1  80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...   \n2  معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...   \n3  مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...   \n4  کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...   \n5  فلم بیشتر پسند آیا۔ دوسرے جائزوں بتایا اچھی کا...   \n6  ٹھیک ، آسکر مستحق ہے۔ گولڈن گلوب۔ ایوارڈ ، معا...   \n7  سائنس فائی چینل دیکھا۔ آیا۔ فلم دلچسپی برقرار ...   \n8  فلم ناقص مووی تھی۔ پلاٹ خراب کامیڈی \"کوشش\" خرا...   \n9  ایم ایم فرینک سناتراس بدترین فلموں چاہئے۔ اوڈ ...   \n\n                                     lemmatized_text  \n0  80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...  \n1  80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...  \n2  معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...  \n3  مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...  \n4  کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...  \n5  فلم بیشتر پسند آیا۔ دوسرے جائزوں بتایا اچھی کا...  \n6  ٹھیک ، آسکر مستحق ہے۔ گولڈن گلوب۔ ایوارڈ ، معا...  \n7  سائنس فائی چینل دیکھا۔ آیا۔ فلم دلچسپی برقرار ...  \n8  فلم ناقص مووی تھی۔ پلاٹ خراب کامیڈی \"کوشش\" خرا...  \n9  ایم ایم فرینک سناتراس بدترین فلموں چاہئے۔ اوڈ ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>lemmatized_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...</td>\n      <td>80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...</td>\n      <td>80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...</td>\n      <td>معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...</td>\n      <td>مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...</td>\n      <td>کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>فلم بیشتر پسند آیا۔ دوسرے جائزوں بتایا اچھی کا...</td>\n      <td>فلم بیشتر پسند آیا۔ دوسرے جائزوں بتایا اچھی کا...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ٹھیک ، آسکر مستحق ہے۔ گولڈن گلوب۔ ایوارڈ ، معا...</td>\n      <td>ٹھیک ، آسکر مستحق ہے۔ گولڈن گلوب۔ ایوارڈ ، معا...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>سائنس فائی چینل دیکھا۔ آیا۔ فلم دلچسپی برقرار ...</td>\n      <td>سائنس فائی چینل دیکھا۔ آیا۔ فلم دلچسپی برقرار ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>فلم ناقص مووی تھی۔ پلاٹ خراب کامیڈی \"کوشش\" خرا...</td>\n      <td>فلم ناقص مووی تھی۔ پلاٹ خراب کامیڈی \"کوشش\" خرا...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ایم ایم فرینک سناتراس بدترین فلموں چاہئے۔ اوڈ ...</td>\n      <td>ایم ایم فرینک سناتراس بدترین فلموں چاہئے۔ اوڈ ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have cleansed text in lemmatized_text and encoded version of sentiment column as encoded_sentiments.\n\nData is prepared for the Modeling.","metadata":{}},{"cell_type":"markdown","source":"### 4.3 Train Test Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(df['lemmatized_text'], df['encoded_sentiments'], test_size = 0.30, random_state = 7, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:04:49.191443Z","iopub.execute_input":"2024-07-11T03:04:49.191996Z","iopub.status.idle":"2024-07-11T03:04:49.217068Z","shell.execute_reply.started":"2024-07-11T03:04:49.191950Z","shell.execute_reply":"2024-07-11T03:04:49.215455Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print('Shape of X_train', X_train.shape)\nprint('Shape of X_test', X_test.shape)\nprint('Shape of Y_train', Y_train.shape)\nprint('Shape of Y_test', Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:04:56.425174Z","iopub.execute_input":"2024-07-11T03:04:56.425629Z","iopub.status.idle":"2024-07-11T03:04:56.434589Z","shell.execute_reply.started":"2024-07-11T03:04:56.425589Z","shell.execute_reply":"2024-07-11T03:04:56.432879Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Shape of X_train (35000,)\nShape of X_test (15000,)\nShape of Y_train (35000,)\nShape of Y_test (15000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4.4 TF - IDF Vectorization\n\nTF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","metadata":{}},{"cell_type":"code","source":"max_feature_num = 50000\nvectorizer = TfidfVectorizer(max_features=max_feature_num)\ntrain_vecs = vectorizer.fit_transform(X_train)\ntest_vecs = TfidfVectorizer(max_features=max_feature_num, vocabulary=vectorizer.vocabulary_).fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:04:59.532464Z","iopub.execute_input":"2024-07-11T03:04:59.532904Z","iopub.status.idle":"2024-07-11T03:05:10.120297Z","shell.execute_reply.started":"2024-07-11T03:04:59.532868Z","shell.execute_reply":"2024-07-11T03:05:10.118976Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# check the dimensions of feature vectors\ntrain_vecs.shape, test_vecs.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:34.572543Z","iopub.execute_input":"2024-07-11T03:10:34.573023Z","iopub.status.idle":"2024-07-11T03:10:34.580838Z","shell.execute_reply.started":"2024-07-11T03:10:34.572986Z","shell.execute_reply":"2024-07-11T03:10:34.579360Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((35000, 50000), (15000, 50000))"},"metadata":{}}]},{"cell_type":"markdown","source":"## 5. Machine Learning Modeling\n\nWe will apply multiple machine learning models and compare the accuracies.","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Support Vector Machine Classifier","metadata":{}},{"cell_type":"code","source":"def SVM_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    SVM = svm.LinearSVC(max_iter=100)\n    SVM.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionSVM = SVM.predict(test_vecs)\n    return classification_report(test_predictionSVM, Y_test), confusion_matrix(test_predictionSVM, Y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:38.954640Z","iopub.execute_input":"2024-07-11T03:10:38.955090Z","iopub.status.idle":"2024-07-11T03:10:38.962039Z","shell.execute_reply.started":"2024-07-11T03:10:38.955053Z","shell.execute_reply":"2024-07-11T03:10:38.960468Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"def LR_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    LR = LogisticRegression()\n    LR.fit(train_vecs, Y_train)\n\n    # testing\n    test_predictionLR = LR.predict(test_vecs)\n    return classification_report(test_predictionLR, Y_test) , confusion_matrix(test_predictionLR, Y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:43.619780Z","iopub.execute_input":"2024-07-11T03:10:43.620272Z","iopub.status.idle":"2024-07-11T03:10:43.627968Z","shell.execute_reply.started":"2024-07-11T03:10:43.620230Z","shell.execute_reply":"2024-07-11T03:10:43.626451Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"def DT_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    DT = DecisionTreeClassifier(max_depth = 9, random_state = 23 )\n    DT.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionDT = DT.predict(test_vecs)\n    return classification_report(test_predictionDT, Y_test), confusion_matrix(test_predictionDT, Y_test) ","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:46.077267Z","iopub.execute_input":"2024-07-11T03:10:46.077711Z","iopub.status.idle":"2024-07-11T03:10:46.084451Z","shell.execute_reply.started":"2024-07-11T03:10:46.077664Z","shell.execute_reply":"2024-07-11T03:10:46.083135Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### 5.4 Xgboost Classifier","metadata":{}},{"cell_type":"code","source":"def XGB_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    XGB = xgb.XGBClassifier(colsample_bytree = 0.2, learning_rate = 0.01, n_estimators = 100)\n    XGB.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionXGB = XGB.predict(test_vecs)\n    return classification_report(test_predictionXGB, Y_test), confusion_matrix(test_predictionXGB, Y_test)  \n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:49.220754Z","iopub.execute_input":"2024-07-11T03:10:49.221194Z","iopub.status.idle":"2024-07-11T03:10:49.227693Z","shell.execute_reply.started":"2024-07-11T03:10:49.221159Z","shell.execute_reply":"2024-07-11T03:10:49.226586Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### 5.5 Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"def RF_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    RF = RandomForestClassifier(n_estimators = 450, max_depth=9, random_state=43)\n    RF.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionRF = RF.predict(test_vecs)\n    return classification_report(test_predictionRF, Y_test), confusion_matrix(test_predictionRF, Y_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:51.082361Z","iopub.execute_input":"2024-07-11T03:10:51.082771Z","iopub.status.idle":"2024-07-11T03:10:51.089425Z","shell.execute_reply.started":"2024-07-11T03:10:51.082727Z","shell.execute_reply":"2024-07-11T03:10:51.087965Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## 6. Results - UrduHack and TF-IDF Vectorizer ","metadata":{}},{"cell_type":"code","source":"class_report , conf_matrix = SVM_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of SVM CLASSIFIER on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:10:54.064401Z","iopub.execute_input":"2024-07-11T03:10:54.064881Z","iopub.status.idle":"2024-07-11T03:10:55.412125Z","shell.execute_reply.started":"2024-07-11T03:10:54.064801Z","shell.execute_reply":"2024-07-11T03:10:55.411024Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Results of SVM CLASSIFIER on TF-IDF Vectorizer\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.87      0.86      7408\n           1       0.87      0.86      0.87      7592\n\n    accuracy                           0.86     15000\n   macro avg       0.86      0.86      0.86     15000\nweighted avg       0.86      0.86      0.86     15000\n\n[[6421  987]\n [1049 6543]]\n","output_type":"stream"}]},{"cell_type":"code","source":"class_report , conf_matrix = LR_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Logistic Regression Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:13:54.555956Z","iopub.execute_input":"2024-07-11T03:13:54.556443Z","iopub.status.idle":"2024-07-11T03:13:59.096314Z","shell.execute_reply.started":"2024-07-11T03:13:54.556401Z","shell.execute_reply":"2024-07-11T03:13:59.095026Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Results of Logistic Regression Classifier on TF-IDF Vectorizer\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.88      0.87      7350\n           1       0.88      0.87      0.87      7650\n\n    accuracy                           0.87     15000\n   macro avg       0.87      0.87      0.87     15000\nweighted avg       0.87      0.87      0.87     15000\n\n[[6444  906]\n [1026 6624]]\n","output_type":"stream"}]},{"cell_type":"code","source":"class_report , conf_matrix = DT_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Decision Tree Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:14:32.583061Z","iopub.execute_input":"2024-07-11T03:14:32.583458Z","iopub.status.idle":"2024-07-11T03:14:41.465465Z","shell.execute_reply.started":"2024-07-11T03:14:32.583425Z","shell.execute_reply":"2024-07-11T03:14:41.464192Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Results of Decision Tree Classifier on TF-IDF Vectorizer\n\n              precision    recall  f1-score   support\n\n           0       0.56      0.81      0.66      5220\n           1       0.87      0.67      0.75      9780\n\n    accuracy                           0.72     15000\n   macro avg       0.72      0.74      0.71     15000\nweighted avg       0.76      0.72      0.72     15000\n\n[[4218 1002]\n [3252 6528]]\n","output_type":"stream"}]},{"cell_type":"code","source":"class_report , conf_matrix = XGB_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Xgboost Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:15:18.710936Z","iopub.execute_input":"2024-07-11T03:15:18.711425Z","iopub.status.idle":"2024-07-11T03:15:45.385119Z","shell.execute_reply.started":"2024-07-11T03:15:18.711383Z","shell.execute_reply":"2024-07-11T03:15:45.383890Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[03:15:21] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nResults of Xgboost Classifier on TF-IDF Vectorizer\n\n              precision    recall  f1-score   support\n\n           0       0.74      0.82      0.78      6731\n           1       0.84      0.76      0.80      8269\n\n    accuracy                           0.79     15000\n   macro avg       0.79      0.79      0.79     15000\nweighted avg       0.79      0.79      0.79     15000\n\n[[5517 1214]\n [1953 6316]]\n","output_type":"stream"}]},{"cell_type":"code","source":"class_report , conf_matrix = RF_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Random Forest Classifier on TF-IDF Vectorizer\\n')\nprint(class_report)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:16:21.634950Z","iopub.execute_input":"2024-07-11T03:16:21.635433Z","iopub.status.idle":"2024-07-11T03:16:40.462456Z","shell.execute_reply.started":"2024-07-11T03:16:21.635394Z","shell.execute_reply":"2024-07-11T03:16:40.461022Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Results of Random Forest Classifier on TF-IDF Vectorizer\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.85      0.82      7072\n           1       0.86      0.81      0.83      7928\n\n    accuracy                           0.83     15000\n   macro avg       0.83      0.83      0.83     15000\nweighted avg       0.83      0.83      0.83     15000\n\n[[5989 1083]\n [1481 6447]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":" ## 7. Sentiment Analysis using Word to Vector and Deep Learning ","metadata":{}},{"cell_type":"code","source":"# Make copy of Dataset to prepare for Word2Vector\ndf_w2v = df.copy() ","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:36:52.162451Z","iopub.execute_input":"2024-07-11T03:36:52.162904Z","iopub.status.idle":"2024-07-11T03:36:52.176858Z","shell.execute_reply.started":"2024-07-11T03:36:52.162866Z","shell.execute_reply":"2024-07-11T03:36:52.175293Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df_w2v.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:36:56.075215Z","iopub.execute_input":"2024-07-11T03:36:56.075692Z","iopub.status.idle":"2024-07-11T03:36:56.095019Z","shell.execute_reply.started":"2024-07-11T03:36:56.075650Z","shell.execute_reply":"2024-07-11T03:36:56.093367Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  \\\n0  80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...  positive   \n1  80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...  negative   \n2  معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...  positive   \n3  مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...  negative   \n4  کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...  positive   \n\n   encoded_sentiments                                    lemmatized_text  \n0                   1  80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...  \n1                   0  80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...  \n2                   1  معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...  \n3                   0  مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...  \n4                   1  کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>encoded_sentiments</th>\n      <th>lemmatized_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>80 دہائی وسط کیبل گائیڈ (اسکائینجر ہنٹ پہلو اپ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>80 دہائی انسپکٹر گیجٹ کارٹون پسند ، فلم دیکھنے...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>معاشرے حالت تعجب والد پیدا البرٹ ٹی فٹزجیرالڈ ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>مفید البرٹ پیون ردی ٹوکری گریڈ زیڈ جلدی۔ ٹم تھ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>کولمبو ہدایتکاری کیریئر ابتدائی وقت اسٹیون اسپ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 7.1 Tokenize the Text using Spacy Urdu Tokenizer","metadata":{}},{"cell_type":"code","source":"import spacy","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:37:58.391598Z","iopub.execute_input":"2024-07-11T03:37:58.392124Z","iopub.status.idle":"2024-07-11T03:37:58.397859Z","shell.execute_reply.started":"2024-07-11T03:37:58.392077Z","shell.execute_reply":"2024-07-11T03:37:58.396014Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def tokenizer(str):\n    nlp = spacy.blank('ur')\n    doc = nlp.tokenizer(str)\n    return [i.text for i in doc]\ndf_w2v[\"tokens\"] = df_w2v[\"lemmatized_text\"].apply(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T03:38:01.510504Z","iopub.execute_input":"2024-07-11T03:38:01.510980Z","iopub.status.idle":"2024-07-11T03:57:54.100291Z","shell.execute_reply.started":"2024-07-11T03:38:01.510939Z","shell.execute_reply":"2024-07-11T03:57:54.099093Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"We have used spacy tokenizer for urdu text. Urduhack library also provides tokenizer but its bit slow so we used this.","metadata":{}},{"cell_type":"markdown","source":"### 7.2 Word to Vector Model\nWord2Vec consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer, one hidden layer and one output layer.","metadata":{}},{"cell_type":"code","source":"import gensim\n\nmodel_word2vec = gensim.models.Word2Vec(sentences=df_w2v[\"tokens\"], size=128, window=5, workers=10, min_count = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets Check the quality of embeddings generated.","metadata":{}},{"cell_type":"code","source":"model_word2vec.wv.most_similar(\"مرد\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_word2vec.wv.most_similar(\"عورت\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_word2vec.wv.most_similar(\"خوفناک\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_word2vec.wv.most_similar(\"فلم\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_word2vec.wv.most_similar(\"کارٹون\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.3 Embedding Layer Preparation","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = len(model_word2vec.wv.vocab)\nDIMENSIONS = 128\nMAX_LEN = max([len(x) for x in df_w2v[\"tokens\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_SIZE, DIMENSIONS, MAX_LEN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The VOCAB_SIZE shows the size of vocabulary.\n* We set the size of dimension to 128 to reduce the dimensions of data.\n* The MAX_LEN represents the Maximum length of Sentence in dataset.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntoken = Tokenizer()\ntoken.fit_on_texts(df_w2v[\"tokens\"])\nencoded = token.texts_to_sequences(df_w2v[\"tokens\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We used Keras Tokenizer to tokenize the data and made text sequences.","metadata":{}},{"cell_type":"code","source":"words2vec_matrix = np.zeros((VOCAB_SIZE+1,DIMENSIONS))\nfor word, index in token.word_index.items():\n    try:\n        words2vec_matrix[index] = model_word2vec.wv[word]\n    except:\n        print(index, word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntrain_vectors = tf.keras.preprocessing.sequence.pad_sequences(encoded,padding='post',dtype=int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label = df_w2v.encoded_sentiments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_label[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the dataset into train and test data.","metadata":{}},{"cell_type":"code","source":"(train_sentences,test_sentences, train_tags, test_tags) = train_test_split(train_vectors, train_label, test_size=0.2, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.4 1D Convolutional Network\nWe will build convolutional network to build the model. Conv layer are used to extract the feature from data.","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.layers as Layers\nimport tensorflow.keras.activations as Actications\nimport tensorflow.keras.models as Models\nfrom tensorflow.keras.optimizers import Adam, Optimizer, SGD\nimport tensorflow.keras.initializers as Init\nfrom tensorflow.keras import regularizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Models.Sequential()\n\nmodel.add(Layers.Embedding(VOCAB_SIZE+1,DIMENSIONS,\n                           embeddings_initializer = Init.Constant(words2vec_matrix),\n                           input_length=MAX_LEN, trainable=False ))\n\nmodel.add(Layers.Conv1D(512, 5, activation=\"relu\"))\nmodel.add(Layers.MaxPooling1D(5))\n\nmodel.add(Layers.Conv1D(256, 5, activation=\"relu\"))\nmodel.add(Layers.MaxPooling1D(5))\n\nmodel.add(Layers.Conv1D(128, 5, activation=\"relu\"))\nmodel.add(Layers.Dropout(0.3))\nmodel.add(Layers.MaxPooling1D(3))\n\nmodel.add(Layers.Conv1D(64, 3, activation=\"relu\"))\nmodel.add(Layers.Dropout(0.3))\nmodel.add(Layers.MaxPooling1D(3))\n\nmodel.add(Layers.Conv1D(32, 3, activation=\"relu\"))\nmodel.add(Layers.Dropout(0.3))\n\nmodel.add(Layers.Flatten())\n\nmodel.add(Layers.Dense(32, activation='relu', kernel_regularizer = regularizers.l2(1e-4)))\nmodel.add(Layers.Dropout(0.6))\n\nmodel.add(Layers.Dense(1,activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONV_NET = model.fit( train_sentences, train_tags, epochs=10, validation_split=0.20 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(CONV_NET.history['accuracy'])\nplt.plot(CONV_NET.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model start overfitting after 5 epochs.","metadata":{}},{"cell_type":"code","source":"plt.plot(CONV_NET.history['loss'])\nplt.plot(CONV_NET.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Validation Loss starts increasing after 5 epochs.","metadata":{}},{"cell_type":"code","source":"print(classification_report(model.predict(test_sentences).round(), test_tags))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.5 Long Short Term Memory\nAs LSTM models works good on text data, as they considered the sequences of inputs to make predictions. We will try LSTM Model on this data. ","metadata":{}},{"cell_type":"code","source":"lstm = Models.Sequential()\n\nlstm.add(Layers.Embedding(VOCAB_SIZE+1,DIMENSIONS,\n                          embeddings_initializer = Init.Constant(words2vec_matrix),\n                          input_length=MAX_LEN, trainable=False ))\n\nlstm.add(Layers.Bidirectional(Layers.LSTM(256, activation='tanh')))\n\nlstm.add(Layers.Dense(128, activation='tanh'))\nlstm.add(Layers.Dropout(0.3))\n\nlstm.add(Layers.Dense(64, activation='tanh'))\nlstm.add(Layers.Dropout(0.3))\n\nlstm.add(Layers.Dense(1, activation='sigmoid'))\n\nlstm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n \n\nlstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nes_callback = EarlyStopping(monitor='val_loss', patience=3) \nLSTM_NET = lstm.fit(train_sentences, train_tags, epochs=10, validation_split=0.2, callbacks=[es_callback], shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(LSTM_NET.history['accuracy'])\nplt.plot(LSTM_NET.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(LSTM_NET.history['loss'])\nplt.plot(LSTM_NET.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lstm model shows same trend, validation loss starts increasing after 5 epochs.","metadata":{}},{"cell_type":"code","source":"print(classification_report(lstm.predict(test_sentences).round(), test_tags))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Conclusion\n\nWe have preprocessed the text data using urduhack library and applied multiple algorithms on this dataset and achieved maximum accuracy of 87 percent using Logistic Regression. In a another notebook we have acieved 88 percent accuracy using SVM but after preprocessing using urduhack the accuracy falls down. So we can conclude that the urduhack text processing preprocess the text quite well but also affects the accuracy.\n\n\nThen we build the Word to Vector embeddings and build deep learning models. We build 2 models, LSTM and Convolutional Network and compare their results and noticed that the LSTM works good on this data and achieved the accuracy of 83 percent. We also achieved the same accuracy with ConvNet but LSTM is more stable in results.","metadata":{}},{"cell_type":"markdown","source":"## 9. Future Work\n\nIn coming days, we will also build Glove Embeddings for this data and train Deep Learning Models.","metadata":{}}]}